"use strict";(globalThis.webpackChunkweb_docs=globalThis.webpackChunkweb_docs||[]).push([[5189],{1585:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"causal-post-psm_deepdive","metadata":{"permalink":"/allye-doc/blog/causal-post-psm_deepdive","editUrl":"https://github.com/Llenar70/allye-doc/tree/main/web-docs/blog/2026-02-08-PSM_deepdive.md","source":"@site/blog/2026-02-08-PSM_deepdive.md","title":"Beyond \\"Just fit()\\": Rethinking PSM for Robust Causal Inference in Production","description":"Welcome to the world of Causal Inference.","date":"2026-02-08T00:00:00.000Z","tags":[{"inline":false,"label":"Causal inference","permalink":"/allye-doc/blog/tags/causal-inference","description":"Causal inference posts"},{"inline":false,"label":"Allye","permalink":"/allye-doc/blog/tags/allye","description":"Allye posts"},{"inline":false,"label":"Data science","permalink":"/allye-doc/blog/tags/data-science","description":"Data science posts"}],"readingTime":19.78,"hasTruncateMarker":true,"authors":[{"name":"Sho SEKINE","title":"Head of Applied Science at mercari, Principal Data Scientist at Fast Retailing, Co-founder AI Allye","url":"https://www.linkedin.com/in/sho-sekine-831339a3/","page":{"permalink":"/allye-doc/blog/authors/sho"},"socials":{"linkedin":"https://www.linkedin.com/in/sho-sekine-831339a3/","github":"https://github.com/LichtLab"},"imageURL":"https://media.licdn.com/dms/image/v2/C5603AQFBwY_R-HmADQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1643884685389?e=1769644800&v=beta&t=UwqhQSINs8z-g6Jplk9blG_E-waB_kjPBqcicWU8L-I","key":"sho"}],"frontMatter":{"slug":"causal-post-psm_deepdive","title":"Beyond \\"Just fit()\\": Rethinking PSM for Robust Causal Inference in Production","authors":["sho"],"tags":["causal-inference","allye","data-science"]},"unlisted":false,"nextItem":{"title":"Vocational Training Program Really Work?","permalink":"/allye-doc/blog/causal-post-nsw"}},"content":"Welcome to the world of Causal Inference.\\n\\nIf you work with data in a production environment, you\'ve likely heard of **Propensity Score Matching (PSM)**. You may have even implemented it using libraries like `causalinference` or `DoWhy`.\\n\\nWriting the code isn\'t difficult. With the modern Python ecosystem, you can calculate propensity scores, perform matching, and estimate effects (ATE/ATT) with just a few lines of code.\\n\\nBut when asked, **\\"Can we really trust these results?\\"** can you confidently say \\"Yes\\"? If not, you lose credibility.\\nOr, could you answer immediately without breaking a cold sweat if a Staff Data Scientist fired these sharp questions at you?\\n\\n*   \\"If you change the random seed, does the result flip from positive to negative?\\"\\n*   \\"Are these matches actually similar? Are you forcing pairs?\\"\\n*   \\"How does the conclusion change if you tighten the caliper slightly?\\"\\n*   \\"What are the characteristics of the data that was excluded (trimmed)?\\"\\n\\nIn this blog, we will thoroughly rethink the \\"classical\\" method of PSM from the perspective of modern production data science. We will also dig deep into the philosophy and specifications of why Allye\'s PSM Widget is designed not just as a calculation tool, but as a **\\"cockpit for protecting analysis quality.\\"**\\n\\nThis should serve as a \\"map of the field\\" for beginners and an \\"implementation answer key\\" for experts.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## 1. The Data Scientist\'s Dilemma: Diagnostic Cost > Computational Cost\\n\\nThe difficulty of causal inference in practice lies not in the computational cost, but in the **\\"diagnostic cost.\\"**\\n\\nThe moment you decide to use PSM because \\"we can\'t do an A/B test (RCT), so let\'s analyze observational data,\\" a Data Scientist wanders into a \\"labyrinth of questions\\":\\n\\n### The Never-Ending Checklist\\n\\n1.  **Covariate Balance Check:**\\n    \\"Is the Love Plot (ASAM) clean? Did all variables fall below the threshold (SMD < 0.1)? If not, should I change the model or drop variables?\\"\\n2.  **Common Support Verification:**\\n    \\"Do the propensity score distributions overlap? Are samples with extreme scores (0.99 or 0.01) distorting the results?\\"\\n3.  **Robustness Guarantee:**\\n    \\"Does the result depend on the random seed? Did we get significance with seed=42 but lose it with seed=123?\\"\\n    \\"What about the influence of unobserved confounders?\\"\\n4.  **Sensitivity Analysis:**\\n    \\"If I change the caliper from 0.1 to 0.05, how does the number of matches decrease and how does the effect size change? How strict do I need to be to get closer to the \'truth\'?\\"\\n\\nIf you try to verify these seriously in Python, even though the core analysis `model.fit()` is just one line, the **\\"boilerplate code for verification, visualization, preprocessing, exception handling, and result formatting\\"** balloons to hundreds of lines.\\n\\n### The \\"Friday Night\\" Temptation\\n\\nOn a Friday night with a deadline looming, looking at massive warning logs (like ConvergenceWarning), a devil whispers:\\n\\"Just `fit()` it, and if no errors pop up, let\'s call it OK. Let\'s pretend... we didn\'t see the fine-tuning of the balance.\\"\\n\\nIf you want to remain a top-tier data scientist, you shouldn\'t run from this \\"gritty rigor.\\" However, writing hundreds of lines of verification code by hand every time is also unrealistic.\\nAllye\'s PSM Widget was designed to solve this dilemma and execute **\\"gritty rigor\\" with \\"overwhelming speed.\\"**\\n\\n---\\n\\n## 2. Intuitive Understanding: Matching as \\"Finding Twins\\"\\n\\nBefore getting into the details of the specifications, let\'s intuitively review how PSM works. Without using formulas, it is **\\"searching for destined twins.\\"**\\n\\n### Why Simple Comparison Doesn\'t Work\\n\\nSuppose you want to know the effect of a \\"discount coupon\\" on an EC site.\\nComparing the average purchase amount of those who received the coupon (Treated) and those who didn\'t (Control) is meaningless.\\n\\nBecause coupons weren\'t distributed randomly.\\nThey might have been given preferentially to \\"people who have shopped often in the past (loyal users).\\" In that case, they would have shopped a lot whether they had a coupon or not. This is **Selection Bias**.\\n\\n### Approach to Counterfactuals\\n\\nWhat we really want to know is the difference:\\n\\"If Mr. A, who received a coupon, **had not received a coupon (counterfactual)**, how much would he have spent?\\"\\n\\nHowever, unless we have a time machine, the real Mr. A has already received the coupon, so we cannot observe \\"Mr. A in the world line where he didn\'t receive it.\\"\\n\\nThis is where PSM comes in.\\nFrom the entire data, we find Mr. B who **\\"is exactly like Mr. A in age, gender, past purchase history, residence, etc., but happened not to receive a coupon,\\"** and pair them up.\\n\\nThis Mr. B is effectively \\"Mr. A\'s twin (proxy).\\"\\nIf we compare this pair, the difference is likely due to the \\"effect of the coupon\\" rather than \\"personal qualities.\\" Doing this for the entire data is PSM.\\n\\n### Hidden Pitfalls\\n\\nHowever, reality is not that simple.\\n\\n*   **Forced Pairing:**\\n    What if there is no one like Mr. A in the control group? If you forcibly bring in \\"Mr. C who is somewhat similar (but actually not very similar),\\" the results will be distorted. Checking \\"Common Support\\" prevents this.\\n*   **Just Happened to be Similar:**\\n    What if you chose the most similar person, but that Mr. B\'s data happened to be a measurement error (noise) or an outlier? 1-to-1 matching is weak against such noise.\\n\\nThat is why **\\"the courage not to match if they are not similar (Caliper)\\"** and **\\"checking if we rely too much on specific partners (Robustness Check)\\"** are essential.\\n\\n---\\n\\n## 3. Deep Dive into Allye PSM Widget Specifications\\n\\nFrom here, I will explain the specific specifications of Allye\'s PSM Widget and the philosophy behind them. I will unravel why so many setting items are necessary from a practical perspective.\\n\\n### 3-1. Consistency of Preprocessing and Model Estimation\\n\\nThe quality of PSM is more than half determined by the stage before matching, that is, \\"Propensity Score Estimation.\\"\\nAllye\'s Widget strictly manages this process internally, which tends to become a black box.\\n\\n*   **Missing Value Handling (Listwise Deletion):**\\n    Automatically detects missing values (NaN) in the selected covariates and excludes them row by row. This maintains consistency between score estimation and matching.\\n*   **Categorical Variable Encoding:**\\n    Automatically performs One-hot encoding. Real-world data contains a lot of categorical variables like \\"Gender,\\" \\"Prefecture,\\" and \\"Membership Rank,\\" but this saves the trouble of manually creating dummy variables.\\n*   **Model Estimation (Logistic Regression):**\\n    Logistic regression (L1/L2 regularization supported) is used to calculate propensity scores.\\n    *   *Why Logistic Regression?*\\n        Recently, there are methods to output propensity scores with Random Forest or LightGBM, but Allye emphasizes interpretability and uses logistic regression as the default. This is because you can check \\"which variables are effective for the treatment (coupon grant)\\" with coefficients, making it easy to audit **\\"whether the model is capturing confounding correctly in the first place.\\"**\\n        Metrics like AUC and LogLoss are also automatically calculated, and if the model fit itself is poor (e.g., AUC around 0.5), you can immediately judge that it is a problem before matching.\\n\\n### 3-2. Matching Target: The \\"Direction\\" of ATT and ATC\\n\\nEven if we say \\"treatment effect,\\" there is actually a direction. Many beginners stumble here. In Allye, you can clearly switch the `Matching Target` as follows:\\n\\n*   **ATT (Average Treatment Effect on the Treated):**\\n    *   **Definition:** Makes \\"people who actually received the treatment (Treated)\\" the main characters (anchors) and looks for \\"people who did not receive the treatment (Control)\\" who are similar to them.\\n    *   **Business Question:** \\"How much did people who used the coupon gain compared to if they hadn\'t used it?\\"\\n    *   **Frequency in Practice:** Highest. Usually used for verifying the effect of measures.\\n\\n*   **ATC (Average Treatment Effect on the Controls):**\\n    *   **Definition:** Makes \\"people who did not receive the treatment (Control)\\" the main characters and looks for \\"people who received the treatment (Treated)\\" who are similar to them.\\n    *   **Business Question:** \\"If people who didn\'t use the coupon had used it, what would have happened?\\"\\n    *   **Significance in Practice:** Can be used for \\"discovering potential demand.\\" For example, finding a segment that is currently outside the target of the measure but would show effects if targeted.\\n\\nThis is not just a difference in labels. It fundamentally changes the behavior of the algorithm: **\\"Which group to keep in full, and which group to sample from.\\"**\\nIf you analyze without being conscious of this, you will end up giving an answer that deviates from the business question (Estimand mismatch).\\n\\n### 3-3. The Courage to \\"Give Up If Not Similar\\": Caliper\\n\\nThe default of many libraries (Nearest Neighbor) tries to find \\"the closest partner anyway.\\"\\nHowever, matching a person with a propensity score of `0.9` and a person with `0.4` cannot be called \\"twins.\\" They are just \\"strangers.\\"\\n\\nIn Allye, you can intuitively set the **Caliper**.\\nBy imposing a constraint such as \\"only accept partners within a score difference of `0.05` (set in standard deviation units, etc.),\\" the quality of matching (Balance) is guaranteed.\\n\\n*   **Visualization of Trade-off:**\\n    *   Tighten Caliper (smaller value) -> Matching quality increases (Bias decreases), but partners are not found and sample size decreases (Variance increases).\\n    *   Loosen Caliper (larger value) -> Sample size can be secured, but dissimilar partners are mixed in, distorting the estimation result (Bias increases).\\n    \\n    Allye\'s strength is that you can check this trade-off in real-time while moving the slider. It supports the decision: \\"Should we prioritize quality even if the sample size is halved?\\"\\n\\n### 3-4. Obsession with Robustness: Randomize from Top-N (Caliper)\\n\\nThis function is not a \\"convenience feature,\\" but **a specification for Sensitivity Analysis**.\\nThe aim is not to swallow the result of a single matching whole, but to verify **\\"whether the conclusion holds even if the way neighbors are taken changes slightly.\\"**\\n\\nUsually, within the Caliper, the \\"closest one person\\" is selected.\\nHowever, in practice, it is not uncommon for there to be multiple candidates who are similarly close.\\nIf you proceed with one fixed person in this case, there is a risk that the estimation result will rely too much on a specific partner.\\n\\nAllye has an option called **`Randomize from Top-N (Caliper)`**.\\n\\n*   **Behavior:** Instead of fixing the closest one, it **\\"creates a list of Top-N candidates (e.g., top 5) in the neighborhood and randomly selects one (or ratio amount) from them.\\"**\\n\\n#### Implementation Image by Pseudo Code\\n\\n```python\\n# Processing image for each anchor (one person in the treatment group)\\nfor anchor in anchor_units:\\n    # 1. Find candidates within Caliper range\\n    candidates = find_candidates_within_caliper(anchor, caliper_value)\\n    \\n    if len(candidates) == 0:\\n        discard(anchor) # Exclude because no match partner\\n        continue\\n\\n    # 2. Sort by distance\\n    sorted_candidates = sort_by_distance(candidates, anchor)\\n    \\n    # 3. Get Top-N (e.g., top 5)\\n    # These are all \\"sufficiently similar (within Caliper)\\" and \\"top class close\\" people\\n    top_n_pool = sorted_candidates[:Top_N]\\n    \\n    # 4. Randomly choose one from them (This is important!)\\n    match = random.choice(top_n_pool, seed=current_seed)\\n    register_match(anchor, match)\\n```\\n\\n*   **Why is this necessary:**\\n    This allows you to **change the random seed (Seed) and run it many times to see the result distribution**.\\n    This is not just a stability check, but a test to see \\"if it is a problem setting that PSM can handle.\\"\\n    \\n    *   **Case A:** When the Seed was changed, the effect (ATT) changed from `+1000 yen` to `-500 yen`.\\n        *   -> **Judgment:** The PSM approach to this question is a red flag.\\n            The conclusion is unstable against the fluctuation of matching partners, and it is highly likely that it is strongly influenced by unobserved confounding or specification dependence.\\n    *   **Case B:** Even if the Seed was changed 10 times, the effect always stayed between `+900 yen ~ +1100 yen`.\\n        *   -> **Judgment:** It can be said that it is at least robust to \\"fluctuation of neighbor selection.\\"\\n            Then, you can take the order of evaluating the residual risk of unobserved confounding in a separate layer.\\n\\nImplementing and managing this with hand-written code and running loops to see the distribution is quite bone-breaking, but with Allye, you can check it immediately with one checkbox and Seed change.\\n\\n### 3-5. Matching Ratio and Replacement\\n\\n*   **Matching Ratio (1:1, 1:2...):**\\n    How many \\"twins\\" to assign to one anchor.\\n    *   `1:1` has the smallest bias, but much data is discarded.\\n    *   Increasing to `1:3` etc. increases the sample size and decreases variance (increases power). However, the second and third persons are likely \\"less similar than the first person,\\" increasing the risk of increased bias.\\n*   **With Replacement:**\\n    Whether to match a partner who has been matched once with other anchors.\\n    *   If `True`, matching is easy to establish even if the control group is small, but since specific data is used many times, dependence on that data increases.\\n    *   If `False` (without replacement), one control person is used only once. Data independence is high, but if the number of control groups is not sufficient, the number of matches decreases drastically.\\n\\n---\\n\\n## 4. The Hierarchy of Robustness and Unobserved Confounding\\n\\nHere lies the most important conceptual foundation for understanding Allye\'s Widget specifications.\\n\\"Reliability of analysis\\" in practice must be considered in the following three hierarchies. Allye is designed with this hierarchy in mind.\\n\\n### Level 1: Stochastic Robustness\\nThe level of \\"Does the result change every time I calculate?\\"\\nPSM includes random elements (handling ties when distances are exactly the same, sampling, Top-N randomization, etc.).\\nIf the result turns from positive to negative just by changing the Seed, it is a problem before analysis. To clear this, we perform a stress test using the aforementioned `Randomize from Top-N`.\\n\\n### Level 2: Design Robustness\\nThe level of \\"Does it depend on the arbitrariness of parameters?\\"\\n\\"The effect appeared because I set the Caliper to `0.05`, but it disappeared when I set it to `0.01`.\\"\\n\\"There is an effect with 1:1 matching, but no effect with 1:2.\\"\\nIn such cases, the result is likely an \\"artifact of parameter settings.\\"\\nIt is necessary to confirm that the conclusion (direction of effect and trend of magnitude) does not change even if parameters are moved (being insensitive). Allye\'s UI is for performing this parameter sweep at high speed.\\n\\n### Level 3: Unobserved Confounding\\nAnd what remains at the end is the influence of factors that do not exist in the data.\\nFor example, when you want to see the \\"effect of a coupon,\\" what if the data only has \\"gender/age\\" and does not include \\"personal saving orientation\\" or \\"influence of TVCM seen by chance\\"?\\n**This cannot be erased no matter how much you calculate.** PSM can only balance \\"observed variables (covariates).\\"\\n\\n### \\"Purifying\\" Risk and Determining Business \\"Resolve\\"\\n\\nSo, why are we so obsessed with Level 1 and Level 2? Isn\'t it useless if Level 3 (unobserved confounding) cannot be erased?\\n\\nNo, absolutely not.\\n**By thoroughly eliminating \\"noise that can be excluded (Level 1, 2: observation bias and calculation instability),\\" the outline of the \\"pure business risk (Level 3)\\" that remains at the end becomes clear for the first time.**\\n\\nMaking decisions without even adjusting for \\"known biases\\" like age and gender, and with unstable calculation results, is just reckless and negligence of a Data Scientist.\\nHowever, the uncertainty that remains after clearing Level 1 and Level 2 to the limit with Allye is no longer a defect of analysis, but a **\\"strategic risk (bet)\\"** that executives and leaders should bear.\\n\\nWhat Allye provides is not a \\"100% guarantee.\\" It is to draw a **boundary line** saying \\"This is what we can say from the data. Beyond this (the unobserved world) is your challenge.\\"\\nClear the fog of uncertainty, face the remaining risk, and step on the gas. Strict and fast PSM is necessary precisely to determine that \\"resolve.\\"\\n\\n---\\n\\n## 5. Reverse Lookup Guide: Reading Settings by Intent\\n\\nIn practice, parameters should be chosen not by \\"which one to raise or lower,\\" but by **\\"which bias to suppress and what to sacrifice.\\"** I organized the main settings based on intent.\\n\\n| Setting Item | What it Controls | Trend when Raised/ON | What is Sacrificed (Trade-off) |\\n| :--- | :--- | :--- | :--- |\\n| **Matching Method** (Caliper) | Selection of candidates | Strongly suppresses outlier matches (Bias down) | Matches decrease, estimation variance may increase |\\n| **Matching Ratio** (1:1 -> 1:3) | Partners per anchor | Variance decrease due to sample increase (Stabilization) | Easier to include distant partners, Bias up |\\n| **Caliper Value** (0.2 -> 0.05) | Allowed PS difference | Stricter matching as it gets smaller | Significant decrease in sample size |\\n| **Randomize from Top-N** | Randomness in neighborhood | Robustness (stability) of results can be confirmed | Analyst needs evaluation design (Seed Loop etc.) |\\n| **With Replacement** | Reuse of partners | Match possible even with small control group | Excessive dependence on specific data, damage to independence |\\n| **Random Seed** | Reproducibility and Explorability | Complete reproduction with same seed, sensitivity check with change | Interpretation of seed dependence required |\\n| **Trimming** | Common Support | Reduction of Extrapolation risk | Reduction of Target Population |\\n\\n---\\n\\n## 6. Proposed Workflow: A Rebuttal to \\"I Can Just Code This\\"\\n\\nYou might think, \\"I can just write this in Python.\\" True, it is technically possible. However, the following problems occur in the field.\\n\\n1.  **Bloating of Boilerplate:** Preprocessing, matching, visualization, aggregation... if you write these properly, it will be on the scale of 1000 lines.\\n2.  **Breeding Ground for Bugs:** Complex code breeds bugs. Mistakes like \\"logic of replacement was wrong,\\" \\"sort order was reversed,\\" or \\"rows shifted in handling missing values\\" are extremely difficult to discover.\\n3.  **Personalization:** A situation where Person A\'s implementation and Person B\'s implementation behave slightly differently is an organizational nightmare.\\n\\n### Recommended Allye Workflow\\n\\nWith Allye, the following \\"advanced analysis flow\\" becomes standard.\\n\\n1.  **Step 1: Create Baseline (Rough Sketch)**\\n    First, grasp the overall picture with default settings (or looser Caliper). Check \\"Can we separate by propensity score in the first place?\\"\\n2.  **Step 2: Improve Quality (Balancing)**\\n    While looking at the Love Plot, tighten the Caliper and secure the balance of variables (SMD < 0.1). Trim areas where distributions do not overlap.\\n3.  **Step 3: Robustness Check (Stress Test - Stochastic)**\\n    Turn on `Randomize from Top-N` and run with several different Seeds. Confirm that the result (ATT/ATC) does not fluctuate significantly. If it fluctuates here, it is insufficient data volume or model failure.\\n4.  **Step 4: Sensitivity Check (Stress Test - Design)**\\n    Confirm that the conclusion does not change even if Caliper or Ratio is changed slightly. If \\"the effect remains even if tightened,\\" the effect is real.\\n5.  **Step 5: Decision Making**\\n    Only after doing this far, report as \\"what can be said from the data.\\" Note the possibility of unobserved confounding.\\n\\nBeing able to run this loop in a few minutes to tens of minutes is the greatest benefit of using the tool. You can verify at a speed different from the manual loop of correcting code, re-running, and outputting plots.\\n\\n---\\n\\n## Conclusion: A Cockpit for Protecting Analysis Quality\\n\\nPSM is not a magic wand. If the data is bad (if important confounding variables are not taken), no matter how advanced the matching is, it is meaningless. The principle of Garbage In, Garbage Out does not change.\\n\\nHowever, Data Scientists have an obligation to do **\\"the best within the observable range.\\"**\\n\\n*   Did you adjust the balance to the limit?\\n*   Did you eliminate unreasonable matching?\\n*   Did you confirm the robustness of the result?\\n*   Is the result reproducible?\\n\\nAllye\'s PSM Widget is a tool to fill these checklists quickly and surely. By changing \\"time to write code\\" into \\"time to think and verify,\\" your analysis will have more persuasive power and become a solid basis for supporting business decisions.\\n\\nRemove the anxiety of Level 1 (Calculation) and Level 2 (Design), and face Level 3 (Business Risk) dignifiedly.\\nPlease experience **\\"fast, strict, and empathetic\\"** causal inference using Allye.\\n\\n---\\n\\n## Appendix: Allye PSM Widget Detailed Specifications (Implementation Base)\\n\\nFrom here, I will summarize the behavior of the Widget like a specification document so that it can be used for review and operation design.\\nIt is the \\"concrete\\" on implementation against the thought part of the main text.\\n\\n### A-1. Input and Output\\n\\n**Input**\\n\\n- `Data` (Orange Table)\\n\\n**Main Output**\\n\\n- `Matched Data`\\n- `Propensity Scores`\\n- `Balance Report`\\n- On-screen Model Diagnosis (AUC/Accuracy/LogLoss)\\n- On-screen Balance Diagnosis (Love Plot / SMD Table)\\n- On-screen Sample Size (Before/After)\\n\\n### A-2. Calculation Pipeline (Internal Order)\\n\\n1. Pre-sampling (if necessary)\\n2. Missing exclusion and variable formatting\\n3. Propensity score model estimation (Logistic Regression)\\n4. Overlap/Positivity diagnosis\\n5. Apply trimming if necessary\\n6. Execute matching (Nearest/Caliper, ATT/ATC, ratio, replacement)\\n7. Calculate balance index (SMD)\\n8. Calculate IPW if necessary\\n9. Generate output table and report data\\n\\nThis order is fixed, so it is reproducible with the same settings, same data, and same seed.\\n\\n### A-3. Specifications of Major Parameters\\n\\n| Parameter | Representative Value/Range | Meaning in Specification |\\n| :--- | :--- | :--- |\\n| Matching Method | `Nearest Neighbor` / `Caliper` | Rule for selecting match candidates |\\n| Matching Ratio | `1:1` / `1:2` / `1:3` | Number of partners per anchor `M` |\\n| Matching Target | `ATT` / `ATC` | Definition of anchor side (reference side) |\\n| Caliper Value | `0.01 - 1.0` | Allowed PS difference |\\n| With Replacement | `True/False` | Reusability of partners |\\n| Random Seed | `1 - 99999` | Reproducibility / Randomization control |\\n| Randomize from Top-N (Caliper) | `True/False` | Enable random selection within Caliper candidates |\\n| Top-N Candidates | `1 - 1000` | Size of random sampling population `N` |\\n| Sampling Mode | `Manual` / `All` | Presence of pre-sampling |\\n| Manual Max Rows | `1000 - 10,000,000` | Max rows for Manual |\\n| Trimming Mode | `None/Percentile/Overlap/Fixed` | Overlap guarantee strategy |\\n| Trim Percentile | `0.0 - 0.1` | Percentile threshold |\\n| Fixed PS Min/Max | `0.0 - 1.0` | Fixed trimming boundary |\\n| IPW Trim Percentile | `0.0 - 0.1` | Trimming for IPW stabilization |\\n\\n### A-4. Strict Behavior of Randomize from Top-N (Caliper)\\n\\nThis option is **valid only during Caliper**.\\nThe behavior is as follows:\\n\\n1. caliper\u5185\u5019\u88dc\u3092\u62bd\u51fa (Extract candidates within caliper)\\n2. PS\u8ddd\u96e2\u3067\u8fd1\u3044\u9806\u306b\u30bd\u30fc\u30c8 (Sort by PS distance)\\n3. \u4e0a\u4f4d `top-N` \u3092\u5019\u88dc\u30d7\u30fc\u30eb\u5316 (Pool top `top-N`)\\n4. \u305d\u306e\u4e2d\u304b\u3089 `M` \u4ef6\u3092\u30e9\u30f3\u30c0\u30e0\u9078\u629e\uff08`M` \u306f ratio \u7531\u6765\uff09 (Randomly select `M` items from them)\\n\\nSupplement:\\n\\n- Works even with `replacement=False` (Respects reuse prohibition)\\n- When candidates are insufficient, it behaves like \\"take as many as possible\\"\\n- Inconsistencies like `top-N < M` are absorbed internally (Actual extraction population is treated to satisfy at least `M`)\\n\\n### A-5. Design Memo per Matching Method\\n\\n**Nearest Neighbor**\\n\\n- Operation where `With Replacement` is practically forced as UI specification\\n- Easy to match in order of closeness, calculation is fast\\n- Outlier match suppression is weaker than Caliper\\n\\n**Caliper**\\n\\n- Can explicitly state \\"Do not match if not similar\\"\\n- Basically recommended for quality-oriented practice\\n- Easy to verify robustness with `Randomize from Top-N`\\n\\n### A-6. Diagnosis/Warning Design\\n\\nThe Widget returns not only results but also diagnostic information.\\n\\n- Sample size before/after Matching\\n- SMD Before/After\\n- Love Plot\\n- Overlap diagnosis (How much is outside common support)\\n- Warning at low match rate (e.g., consider replacement)\\n- Model convergence warning (if necessary)\\n\\nPlease be sure to check these as a set, not just the \\"effect size\\" alone when making decisions.\\n\\n### A-7. Note on Reproducibility\\n\\n`Random Seed` mainly affects:\\n\\n- Pre-sampling\\n- Anchor processing order\\n- Top-N random selection\\n\\nIn other words, changing the seed is not just a \\"visual random number change\\" but a **stress test for matching design**.\\nConversely, when reporting, it is mandatory to fix the seed and leave reproducible settings.\\n\\n### A-8. Estimand and Display Value Organization\\n\\nIn practice, the following confusion must be avoided:\\n\\n- `ATT/ATC` is the setting of \\"Matching Direction\\"\\n- `ATE` is the \\"Aggregation Definition of Effect Estimation\\"\\n\\nIn review materials, please always declare \\"which estimand was the main target this time\\" at the beginning.\\nWithout this declaration, discussions will almost certainly not mesh.\\n\\n### A-9. Scale of Implementation (Reference)\\n\\nIf you implement equivalent specifications by hand, realistically the following will be required:\\n\\n- Preprocessing and type-safe conversion\\n- Matching algorithm branching\\n- Random number management\\n- Diagnostic visualization\\n- Exception handling\\n- Output formatting\\n- Testing\\n\\nIncluding operational quality, the actual feeling is that it tends to be on the scale of `1,000 to 3,000 lines` (more including tests).\\nWhat determines quality here is not the amount of code itself, but the **analyst\'s design ability (assumptions, diagnosis, robustness design)**.\\n\\n---\\n\\n*You can try [Allye Base](https://www.ai-allye.com/) for free.*"},{"id":"causal-post-nsw","metadata":{"permalink":"/allye-doc/blog/causal-post-nsw","editUrl":"https://github.com/Llenar70/allye-doc/tree/main/web-docs/blog/2026-01-21-Vocational_training_Effect.md","source":"@site/blog/2026-01-21-Vocational_training_Effect.md","title":"Vocational Training Program Really Work?","description":"\\"Does vocational training truly boost participants\' future earnings?\\"","date":"2026-01-21T00:00:00.000Z","tags":[{"inline":false,"label":"Causal inference","permalink":"/allye-doc/blog/tags/causal-inference","description":"Causal inference posts"}],"readingTime":9.49,"hasTruncateMarker":false,"authors":[{"name":"Sho SEKINE","title":"Head of Applied Science at mercari, Principal Data Scientist at Fast Retailing, Co-founder AI Allye","url":"https://www.linkedin.com/in/sho-sekine-831339a3/","page":{"permalink":"/allye-doc/blog/authors/sho"},"socials":{"linkedin":"https://www.linkedin.com/in/sho-sekine-831339a3/","github":"https://github.com/LichtLab"},"imageURL":"https://media.licdn.com/dms/image/v2/C5603AQFBwY_R-HmADQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1643884685389?e=1769644800&v=beta&t=UwqhQSINs8z-g6Jplk9blG_E-waB_kjPBqcicWU8L-I","key":"sho"}],"frontMatter":{"slug":"causal-post-nsw","title":"Vocational Training Program Really Work?","authors":["sho"],"tags":["causal-inference"]},"unlisted":false,"prevItem":{"title":"Beyond \\"Just fit()\\": Rethinking PSM for Robust Causal Inference in Production","permalink":"/allye-doc/blog/causal-post-psm_deepdive"},"nextItem":{"title":"Personalize Email Campaign-MineThatData Challenge","permalink":"/allye-doc/blog/causal-post"}},"content":"\\"Does vocational training truly boost participants\' future earnings?\\"\\n\\nFor policymakers and business leaders, measuring the real impact of such programs is a critical challenge. A simple comparison between participants and non-participants is often misleading\u2014for instance, highly motivated individuals might be more likely to sign up, skewing the results.\\n\\nTo solve this, we need **Causal Inference**.\\n\\nIn this post, we revisit a classic case study based on LaLonde\'s National Supported Work Demonstration (NSW) data. We will move beyond the textbook theory and demonstrate how to strip away bias to uncover the true program effect.\\n\\nToday, let\'s analyze this data using **Allye Pro**.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_cps_mixed_data_analysis.png\').default}\\n    alt=\\"CATE Prediction\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\n\\n### 1. Data Generation\\n\\nThe data is available in the `causaldata` package. We will use it to create a mixed dataset (`nsw_cps_mixed_data`) that combines the experimental treatment group with the observational control group.\\n\\nYou can use the code below to generate the data. Or you can also download csv file from [here](https://raw.githubusercontent.com/Llenar70/allye-doc/main/web-docs/blog/data/nsw_cps_mixed.csv).\\n\\n```python\\nfrom causaldata import nsw_mixtape, cps_mixtape\\nimport pandas as pd\\n\\n# NSW randomized experiment\\ndf_nsw = nsw_mixtape.load_pandas().data.copy()\\n# CPS observational data\\ndf_cps = cps_mixtape.load_pandas().data.copy()\\ncommon_cols = [\\n    \\"age\\", \\"educ\\", \\"black\\", \\"hisp\\", \\"marr\\",\\n    \\"nodegree\\", \\"re74\\", \\"re75\\", \\"re78\\"\\n]\\ndf_cps_use = df_cps[common_cols].copy()\\ndf_cps_use[\\"treat\\"] = 0\\ndf_cps_use[\\"source\\"] = \\"CPS\\"\\n# Select only the treated group from the experimental data\\ndf_nsw_use = df_nsw[df_nsw[\\"treat\\"] == 1][common_cols + [\\"treat\\"]].copy()\\ndf_nsw_use[\\"source\\"] = \\"NSW\\"\\n# Combine them to form a biased dataset\\ndf_mixed = pd.concat(\\n    [df_nsw_use, df_cps_use],\\n    axis=0,\\n    ignore_index=True\\n)\\ndf_mixed[\'treat\'] = df_mixed[\'treat\'].astype(\'category\')\\ndf_mixed.head()\\n```\\n\\nHere is a breakdown of the variables in the dataset:\\n\\n<div style={{fontSize: \'70%\'}}>\\n\\n| Variable | Definition | Role | Details |\\n| :--- | :--- | :--- | :--- |\\n| **treat** | Treatment Indicator | Treatment| **1 = Received Job Training**, **0 = Did not receive**. This is the key variable for our analysis. |\\n| **age** | Age | Covariate | Age of the participant. |\\n| **educ** | Education | Covariate | Years of education completed (e.g., 12 = High School graduate). |\\n| **black** | Black (Dummy) | Covariate| 1 = Black, 0 = Otherwise. |\\n| **hisp** | Hispanic (Dummy) | Covariate| 1 = Hispanic, 0 = Otherwise. |\\n| **marr** | Married (Dummy) | Covariate | 1 = Married, 0 = Single/Other. |\\n| **nodegree** | No Degree (Dummy) | Covariate | 1 = No High School Degree, 0 = Has Degree. Used to identify dropouts. |\\n| **re74** | Real Earnings 1974 | Covariate | **Pre-treatment Income 1**. Indicates economic status before the program. Participants often have low values here. |\\n| **re75** | Real Earnings 1975 | Covariate  | **Pre-treatment Income 2**. Immediate pre-program income. Often zero for participants in this dataset. |\\n| **re78** | Real Earnings 1978 | Outcome | **Post-treatment Income**. The target variable. We want to see if `treat=1` leads to an increase here. |\\n| **source** | Data Source | Metadata | Origin of the record (\'NSW\' for experimental treated, \'CPS\' for observational control). |\\n</div>\\n\\n### 2. A/A Test and Checking Bias in Treatment Effects\\n\\nThe NSW dataset consists of individuals who sought and received vocational training. The `cps_mixtape` data, however, represents a general population sample.\\n\\nThere are likely many underlying factors that motivate someone to seek vocational training. First, let\'s perform a quick A/A Test to check if the two groups are homogeneous.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_aatest.png\').default}\\n    alt=\\"A/A Test Results\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\n<div style={{fontSize: \'70%\'}}>\\n\\n| Variable | Group | Sample Size | Average | 95% CI | Effect \u0394 | Lift (%) | p-value | Significant |\\n|---|---|---|---|---|---|---|---|---|\\n| age | Control | 15992 | 33.23 | [33.05, 33.40] | - | - | - | No |\\n| | Treated | 185 | 25.82 | [24.78, 26.85] | -7.41 | -22.3% | 0.000 | **Yes** |\\n| educ | Control | 15992 | 12.03 | [11.98, 12.07] | - | - | - | No |\\n| | Treated | 185 | 10.35 | [10.05, 10.64] | -1.68 | -14.0% | 0.000 | **Yes** |\\n| black | Control | 15992 | 0.07 | [0.07, 0.08] | - | - | - | No |\\n| | Treated | 185 | 0.84 | [0.79, 0.90] | +0.77 | +1046.7% | 0.000 | **Yes** |\\n| marr | Control | 15992 | 0.71 | [0.70, 0.72] | - | - | - | No |\\n| | Treated | 185 | 0.19 | [0.13, 0.25] | -0.52 | -73.4% | 0.000 | **Yes** |\\n| nodegree | Control | 15992 | 0.30 | [0.29, 0.30] | - | - | - | No |\\n| | Treated | 185 | 0.71 | [0.64, 0.77] | +0.41 | +139.4% | 0.000 | **Yes** |\\n| re74 | Control | 15992 | 14016.80 | [13868.47, 14165.13] | - | - | - | No |\\n| | Treated | 185 | 2095.57 | [1386.75, 2804.39] | -11921.23 | -85.0% | 0.000 | **Yes** |\\n| re75 | Control | 15992 | 13650.80 | [13507.11, 13794.49] | - | - | - | No |\\n| | Treated | 185 | 1532.06 | [1065.09, 1999.02] | -12118.75 | -88.8% | 0.000 | **Yes** |\\n| **re78** | Control | 15992 | 14846.66 | [14697.13, 14996.19] | - | - | - | No |\\n| | Treated | 185 | 6349.14 | [5207.95, 7490.34] | **-8497.52** | -57.2% | 0.000 | **Yes** |\\n\\n</div>\\n\\nThose who received vocational training are generally younger, have lower education levels, and significantly lower pre-training earnings (`re74`, `re75`).\\n\\nJust because the `re78` (earnings in 1978) is higher for the non-treated group doesn\'t mean the training was pointless. It simply suggests that even if the training had a positive effect, it wasn\'t enough to close the massive initial gap between the two groups. The A/B test reports a negative effect of **-$8497.52**, but we cannot conclude this is the causal effect of the intervention due to the severe selection bias.\\n\\n### 3. Propensity Score Matching\\n\\nTo address this bias, we apply **Propensity Score Matching (PSM)**, a standard technique in causal inference.\\n\\nWe select covariates for balancing (e.g., demographics, prior earnings) and choose the outcome variable.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_psm_full_report.png\').default}\\n    alt=\\"PSM Report\\"\\n    style={{ width: \'75%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nLooking at the Love Plot and the balance table, we can see that the discrepancies identified in the A/A test have been successfully mitigated. The matching process has created a control group that is statistically very similar to the treated group.\\n\\nNow, let\'s run an A/B Test on this matched dataset:\\n\\n<div style={{fontSize: \'70%\'}}>\\n\\n| Variable | Group | Sample Size | Average | 95% CI | Effect \u0394 | Lift (%) | p-value | Significant |\\n|---|---|---|---|---|---|---|---|---|\\n| **re78** | Control (0) | 164 | 4564.52 | [3736.96, 5392.07] | - | - | - | No |\\n| | Treated (1) | 164 | 6429.95 | [5227.35, 7632.55] | +1865.43 | +40.9% | 0.012 | **Yes** |\\n\\n</div>\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_abtest.png\').default}\\n    alt=\\"Matched A/B Test\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nWe now estimate a positive effect of **$1865.43**. This difference is statistically significant.\\n\\n### 4. Validation: Checking the Answer Key\\n\\nSince the original NSW dataset is from a Randomized Controlled Trial (RCT), we can calculate the *true* experimental effect by comparing the treated group with the *experimental* control group (not the CPS data). (While there is some slight bias in `nodegree`, the groups are largely balanced.)\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_rct_ab_test.png\').default}\\n    alt=\\"True RCT Effect\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\n**Analysis Settings**\\n- Treatment Variable: `treat`\\n- Control Group: `0`\\n- Test Type: Auto (based on variable type)\\n- Confidence Level: 95%\\n- Multiple Comparison Correction: None\\n\\n<div style={{fontSize: \'70%\'}}>\\n\\n| Outcome     | Group      | Sample | Average   | Abs CI                | Effect \u0394   | Lift (%) | Effect CI (\u0394) | p-value | Significant |\\n|-------------|------------|--------|-----------|-----------------------|------------|----------|---------------|---------|-------------|\\n| **age**     | Control| 260    | 25.05     | [24.19, 25.92]        | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 25.82     | [24.78, 26.85]        | +0.76      | +3.0%    | -             | 0.266   | No          |\\n| **educ**    | Control| 260    | 10.09     | [9.89, 10.29]         | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 10.35     | [10.05, 10.64]        | +0.26      | +2.6%    | -             | 0.150   | No          |\\n| **black**   | Control| 260    | 0.83      | [0.78, 0.87]          | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 0.84      | [0.79, 0.90]          | +0.02      | +2.0%    | -             | 0.647   | No          |\\n| **hisp**    | Control| 260    | 0.11      | [0.07, 0.15]          | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 0.06      | [0.03, 0.09]          | -0.05      | -44.8%   | -             | 0.064   | No          |\\n| **marr**    | Control| 260    | 0.15      | [0.11, 0.20]          | -          | -        | -             | -       | No          |\\n|             | Treatment  | 185    | 0.19      | [0.13, 0.25]          | +0.04      | +23.0%   | -             | 0.334   | No          |\\n| **nodegree**| Control| 260    | 0.83      | [0.79, 0.88]          | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 0.71      | [0.64, 0.77]          | -0.13      | -15.2%   | -             | 0.002   | **Yes**         |\\n| **re74**    | Control| 260    | 2107.03   | [1412.41, 2801.65]    | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 2095.57   | [1386.75, 2804.39]    | -11.45     | -0.5%    | -             | 0.982   | No          |\\n| **re75**    | Control| 260    | 1266.91   | [887.97, 1645.85]     | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 1532.06   | [1065.09, 1999.02]    | +265.15    | +20.9%   | -             | 0.385   | No          |\\n| **re78**    | Control| 260    | 4554.80   | [3885.10, 5224.50]    | -          | -        | -             | -       | No          |\\n|             | Treatment | 185    | 6349.14   | [5207.95, 7490.34]    | **+1794.34**   | +39.4%   | -             | 0.008   | **Yes**         |\\n\\n</div>\\n\\nThe true effect is **+$1794.34**. Our PSM estimate of **$1865.43** differs by less than 4%, demonstrating that PSM was able to recover the causal effect with high accuracy from the observational data.\\n\\n### 5. Advanced Topics: Heterogeneous Treatment Effects\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_cate_estimation.png\').default}\\n    alt=\\"CATE Estimation\\"\\n    style={{ maxHeight: \'80vh\', width: \'75%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nUsing machine learning, we can go a step further and estimate the **Conditional Average Treatment Effect (CATE)** for individuals. Given the small sample size and high variance, we\'ll use **LinearDML**, which provides robust CATE estimation.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_linearDML.png\').default}\\n    alt=\\"LinearDML\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nBy averaging the predicted CATE for the treated individuals (`treat = 1`), we can compare this result with our previous average treatment effects.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_mean_cate.png\').default}\\n    alt=\\"Mean CATE\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nThe calculated result is **$1495**. While there is a ~16.7% deviation from the true $1794, it is a massive improvement over the naive observational comparison (-$8497) and provides a directional estimate good enough for decision-making.\\n\\n#### One more tip for the accurate understanding\\n\\nIn the LinearDML report, the factors contributing to CATE showed that both `re74` and `re75` had negative coefficients, with `re74` showing a particularly strong negative correlation.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_effect_model_coef.png\').default}\\n    alt=\\"Effect Model Coefficients\\"\\n    style={{ maxHeight: \'80vh\', width: \'75%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nIt makes intuitive sense that people with higher prior earnings might benefit less from basic vocational training. However, the fact that `re74` (income 4 years prior) had a much stronger correlation than `re75` (income 3 years prior) seemed odd.\\n\\nBefore jumping to conclusions, we should check for **multicollinearity**, as LinearDML (being a linear model) is sensitive to it.\\n\\nChecking the scatter plot and correlation between `re74` and `re75`, we find a high correlation coefficient (r=0.87). The plot also suggests a ceiling effect.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_re74_re75.png\').default}\\n    alt=\\"re74 vs re75\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nThis collinearity might be distorting the coefficients. To fix this, we can filter out the ceiling values as outliers and apply **Principal Component Analysis (PCA)** to `re74` and `re75` to create orthogonal components.\\n*   **PC1:** Positively correlated with both `re74` and `re75` (represents overall income level).\\n*   **PC2:** Represents the difference/variance between the years.\\n\\nRe-running LinearDML with PC1 and PC2 instead of the raw variables yields the following:\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/nsw_pca_linearDML_effect_model.png\').default}\\n    alt=\\"PCA LinearDML\\"\\n    style={{ maxHeight: \'80vh\', width: \'75%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nBoth components still show a negative correlation with CATE, but **PC1** (overall income level) has the strongest negative correlation. This confirms our hypothesis: **Vocational training is less effective for those who already have high earning potential.** It wasn\'t about `re74` specifically, but the general income level.\\n\\nAdditionally, `age` shows a positive correlation, suggesting that older participants (within this demographic) benefited more from the training than younger ones.\\n\\n### 6. Conclusion and Summary\\n\\nOur analysis of the NSW vocational training program revealed several key insights:\\n\\n1.  **Bias Correction:** Simple comparison of observational data led to a misleading negative effect (-$8500). Propensity Score Matching successfully corrected this bias, estimating a positive effect (+$1865) very close to the true experimental benchmark (+$1794).\\n2.  **Targeting Efficiency:** Vocational training budgets and manpower are limited. To maximize effectiveness, our CATE analysis suggests a clear policy direction:\\n    *   **Focus on those with lower prior earnings.** The training has diminishing returns for those with higher baseline income.\\n    *   **Prioritize older applicants.** Within this group, older individuals showed higher treatment effects.\\n\\nSimply looking at post-training income (`re78`) might tempt administrators to select candidates who are likely to earn more anyway (high prior earners). However, our causal analysis proves this would be a mistake\u2014those individuals benefit the least from the program. The true value of the training is maximized by targeting those who need it most.\\n\\n\\n\\n### Data Science Is Fun! Getting It Right Is What Makes It Valuable.\\nAchieve deeper understanding and higher-quality outputs in data science\u2014beyond your peers.\\n*If you want to explore the data yourself, grab the dataset and try reproducing these results in Allye!*\\n\\nYou can try [Allye Base](https://www.ai-allye.com/) for free."},{"id":"causal-post","metadata":{"permalink":"/allye-doc/blog/causal-post","editUrl":"https://github.com/Llenar70/allye-doc/tree/main/web-docs/blog/2026-01-17-Personalize_Email_Campaign.md","source":"@site/blog/2026-01-17-Personalize_Email_Campaign.md","title":"Personalize Email Campaign-MineThatData Challenge","description":"MineThatData E-Mail Analytics And Data Mining Challenge was originally published in 2008 from MineThatData, this dataset invites us to solve a timeless marketing problem: How do we personalize campaigns?","date":"2026-01-17T00:00:00.000Z","tags":[{"inline":false,"label":"Causal inference","permalink":"/allye-doc/blog/tags/causal-inference","description":"Causal inference posts"}],"readingTime":9.55,"hasTruncateMarker":true,"authors":[{"name":"Sho SEKINE","title":"Head of Applied Science at mercari, Principal Data Scientist at Fast Retailing, Co-founder AI Allye","url":"https://www.linkedin.com/in/sho-sekine-831339a3/","page":{"permalink":"/allye-doc/blog/authors/sho"},"socials":{"linkedin":"https://www.linkedin.com/in/sho-sekine-831339a3/","github":"https://github.com/LichtLab"},"imageURL":"https://media.licdn.com/dms/image/v2/C5603AQFBwY_R-HmADQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1643884685389?e=1769644800&v=beta&t=UwqhQSINs8z-g6Jplk9blG_E-waB_kjPBqcicWU8L-I","key":"sho"}],"frontMatter":{"slug":"causal-post","title":"Personalize Email Campaign-MineThatData Challenge","authors":["sho"],"tags":["causal-inference"]},"unlisted":false,"prevItem":{"title":"Vocational Training Program Really Work?","permalink":"/allye-doc/blog/causal-post-nsw"},"nextItem":{"title":"Hello! and Welcome","permalink":"/allye-doc/blog/first-post"}},"content":"MineThatData E-Mail Analytics And Data Mining Challenge was originally published in [2008 from MineThatData](https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html), [this dataset](https://www.kaggle.com/datasets/bofulee/kevin-hillstrom-minethatdata-e-mailanalytics) invites us to solve a timeless marketing problem: **How do we personalize campaigns?**\\n\\nIn this post, we\'ll dive into this dataset using **Causal Inference** to uncover not just *which* campaign worked best, but *why* and *how to improve*.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Data\\n\\nFirst, let\'s get familiar with what we\'re looking at. The dataset contains 64,000 customers who made a purchase within the last 12 months. These customers were part of a randomized email experiment:\\n\\n*   **1/3** received an email featuring **Mens merchandise**.\\n*   **1/3** received an email featuring **Womens merchandise**.\\n*   **1/3** were in the **Control group** (no email).\\n\\nWe have two weeks of tracking data following the campaign to see if these emails actually drove results.\\n\\n### The Attributes\\n\\n\\nHere is a summary of the dataset attributes:\\n\\n| Variable Name          | Description                                                              | Example Values / Notes              |\\n|------------------------|--------------------------------------------------------------------------|-------------------------------------|\\n| **Recency**            | Months since last purchase                                               | 1, 4, 12                            |\\n| **History_Segment**    | Categorical buckets for dollars spent in the past year                   | `$0-$100`, `$100-$200`, etc.        |\\n| **History**            | Actual dollar value spent in the past year                               | 76.50, 340.00                       |\\n| **Mens**               | Purchased Mens merchandise in the past year (1/0)                        | 1 = Yes, 0 = No                     |\\n| **Womens**             | Purchased Womens merchandise in the past year (1/0)                      | 1 = Yes, 0 = No                     |\\n| **Zip_Code**           | Customer location type                                                   | Urban, Suburban, Rural              |\\n| **Newbie**             | New customer in the past 12 months (1/0)                                 | 1 = Yes, 0 = No                     |\\n| **Channel**            | Purchase channel in the past year                                        | Web, Phone, Multichannel            |\\n| **Segment**            | Which campaign the customer received                                     | Mens E-Mail, Womens E-Mail, No E-Mail|\\n\\n**Post-campaign outcome variables (tracked in the 2 weeks after the campaign):**\\n\\n| Variable Name   | Description                                                | Example Values            |\\n|-----------------|------------------------------------------------------------|---------------------------|\\n| **Visit**       | Visited the website in the two weeks after campaign (1/0)  | 1 = Yes, 0 = No           |\\n| **Conversion**  | Purchased merchandise post-campaign (1/0)                  | 1 = Yes, 0 = No           |\\n| **Spend**       | Dollars spent in the two weeks after campaign              | 0.00, 24.99, 140.00       |\\n\\n\\n### The Goal\\nThe challenge poses several questions, but they essentially boil down to this:\\n1.  **Overall Performance**: Did the emails work? Which one was better?\\n2.  **Targeting**: If we could only send emails to the best 10,000 customers, who should they be? Who should we avoid?\\n3.  **The \\"Why\\"**: Can we explain the drivers behind these results?\\n\\nLet\'s see how a causal approach can answer these better than simple averages.\\n\\n---\\n\\n## 1. A/B Testing\\n\\nWe start with the basics. Using the **A/B Test** node for statistical test, we first run a sanity check (A/A Test) to confirm the randomization was valid. Then, we look at the main metrics: **Conversion**, **Visit**, and **Spend**.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/overallAB.png\').default}\\n    alt=\\"Overall A/A & A/B Test\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\n#### Results\\n\\n#### Summary of A/B Test Results\\n\\n| Metric      | Group                | Sample Size | Mean / Rate (%) | 95% CI (Mean/Rate)   | Effect \u0394    | Lift (%)   | p-value | Significant |\\n|-------------|----------------------|-------------|-----------------|----------------------|-------------|------------|---------|-------------|\\n| **Recency** | Mens E-Mail          | 21,307      | 5.77            | [5.73, 5.82]         | +0.02       | +0.4%      | 0.481   | No          |\\n|             | No E-Mail (Control)  | 21,306      | 5.75            | [5.70, 5.80]         | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 5.77            | [5.72, 5.81]         | +0.02       | +0.3%      | 0.593   | No          |\\n| **History** | Mens E-Mail          | 21,307      | 242.84          | [239.34, 246.33]     | +1.95       | +0.8%      | 0.432   | No          |\\n|             | No E-Mail (Control)  | 21,306      | 240.88          | [237.49, 244.28]     | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 242.54          | [239.11, 245.96]     | +1.65       | +0.7%      | 0.501   | No          |\\n| **Mens**    | Mens E-Mail          | 21,307      | 55.1%           | [54.4%, 55.8%]       | -0.2pp      | -0.4%      | 0.643   | No          |\\n|             | No E-Mail (Control)  | 21,306      | 55.3%           | [54.7%, 56.0%]       | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 54.9%           | [54.2%, 55.6%]       | -0.4pp      | -0.8%      | 0.378   | No          |\\n| **Womens**  | Mens E-Mail          | 21,307      | 55.1%           | [54.5%, 55.8%]       | +0.4pp      | +0.7%      | 0.439   | No          |\\n|             | No E-Mail (Control)  | 21,306      | 54.8%           | [54.1%, 55.4%]       | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 55.0%           | [54.3%, 55.7%]       | +0.2pp      | +0.4%      | 0.616   | No          |\\n| **Newbie**  | Mens E-Mail          | 21,307      | 50.2%           | [49.5%, 50.8%]       | -0.0pp      | -0.1%      | 0.934   | No          |\\n|             | No E-Mail (Control)  | 21,306      | 50.2%           | [49.5%, 50.9%]       | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 50.3%           | [49.7%, 51.0%]       | +0.1pp      | +0.3%      | 0.799   | No          |\\n| **Visit**   | Mens E-Mail          | 21,307      | 18.3%           | [17.8%, 18.8%]       | +7.7pp      | +72.1%     | 0.000   | Yes         |\\n|             | No E-Mail (Control)  | 21,306      | 10.6%           | [10.2%, 11.0%]       | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 15.1%           | [14.7%, 15.6%]       | +4.5pp      | +42.6%     | 0.000   | Yes         |\\n| **Conversion** | Mens E-Mail       | 21,307      | 1.3%            | [1.1%, 1.4%]         | +0.7pp      | +118.8%    | 0.000   | Yes         |\\n|             | No E-Mail (Control)  | 21,306      | 0.6%            | [0.5%, 0.7%]         | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 0.9%            | [0.8%, 1.0%]         | +0.3pp      | +54.3%     | 0.000   | Yes         |\\n| **Spend**   | Mens E-Mail          | 21,307      | 1.42            | [1.18, 1.66]         | +0.77       | +117.9%    | 0.000   | Yes         |\\n|             | No E-Mail (Control)  | 21,306      | 0.65            | [0.50, 0.81]         | \u2013           | \u2013          | \u2013       | No          |\\n|             | Womens E-Mail        | 21,387      | 1.08            | [0.87, 1.28]         | +0.42       | +65.0%     | 0.001   | Yes         |\\n\\n> **Note**: \\"pp\\" means \\"percentage points\\" for difference in rates.\\n\\nNo selection bias and both campaigns drove statistically significant lift across all metrics compared to the control group. However, the **Mens campaign** was the clear winner:\\n\\n*   **Mens Campaign**: +$0.77 spend per customer.\\n*   **Womens Campaign**: +$0.42 spend per customer.\\n\\nAt a high level, you might conclude: \\"Great, send the Men\'s email to everyone!\\" But as professionals, let\'s optimize further.\\n\\n---\\n\\n## 2. Beyond Averages: Finding the Best (and Worst) Customers\\n\\nThe challenge asks an interesting question:\\n> *If you could only send emails to the **best 10,000** customers, who would they be? Conversely, who would you **suppress**?*\\n\\nThis is where the job of an analyst splits into two equally important paths:\\n1.  **The Algorithmic Path**: Accurately identifying the top/bottom 10k users to maximize ROI.\\n2.  **The Insight Path**: Explaining *why* these users are the best/worst to stakeholders.\\n\\nIn my experience, mastering the *Algorithmic Path* gets you a bigger bonus. On the other hand, Mastering the *Insight Path* gets you trusted and promoted. The former powers the system; the latter powers the strategy.\\n\\n### Step 1: Estimating CATE\\nTo solve the algorithmic part, we can estimate the **Conditional Average Treatment Effect (CATE)** for each user. This tells us the expected lift (or loss) for a specific individual if they receive the email.\\n\\nIn Allye, we can use the **Causal Forest** to predict CATE for `spend`.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/CATE_prediction.png\').default}\\n    alt=\\"CATE Prediction\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nBy sorting customers based on their predicted CATE, we can easily slice the **Top 10k (Best)** and **Bottom 10k (Worst)**.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/best_customers.png\').default}\\n    alt=\\"List Best Customers\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nThis gives us our target list. But we\'re not done yet. We need to understand *who* these people are.\\n\\n---\\n\\n## 3. The \\"Why\\": Analyzing the Drivers\\n\\nNow we move on to the \\"Insight Path.\\" Since we already have CATE predictions for each individual, it makes sense to dig deeper into what drives these effects. Of course, we should keep in mind that predictions have errors, so it\'s important to use these CATE-based insights as a starting point and then carefully validate them with targeted, stratified-A/B testing.\\n\\nWe can use Regression Analysis or Decision Trees to see which features correlate most strongly with a high CATE.\\n\\n### The Mens Campaign: A Success Story\\nFor the Mens campaign, the analysis highlights three key drivers: **History** (past spend), **Recency** (months since last purchase), and **Channel** (Phone vs Web vs Multichannel).\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/CATE_factors.png\').default}\\n    alt=\\"CATE Factors\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nThe highest lift comes from users who:\\n*   Are heavy spenders with a solid purchase history\\n*   Made a recent purchase (short time since last purchase)\\n*   Shop through **multiple channels** (Multichannel)\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/segment_AB.png\').default}\\n    alt=\\"segment_AB\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nWhen we isolate this segment and re-run the A/B test, the results are staggering. The lift jumps to **+$1.97** (a +584% increase!).\\n\\nConversely, \\"Phone-only\\" customers with low purchase history show almost **no significant lift**. This makes intuitive sense: email is a digital channel. Customers who only buy over the phone may simply not engage with digital marketing.\\n\\n### The Womens Campaign: A Warning Sign\\nThe Womens campaign reveals a more complex\u2014and concerning\u2014story.\\n\\nWhen we analyze the drivers here, **Zip Code** pops up as a significant factor. Specifically, customers in **Suburban** areas reacted differently.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/women_CATE_factor.png\').default}\\n    alt=\\"Women CATE Factor\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nDeep diving into the \\"low performance\\" segment (Suburban, Phone channel, Low spend), we find something alarming: **The No-Email group actually outspent the Email group.**\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/women_low_CATE_segment.png\').default}\\n    alt=\\"Women Low CATE Segment\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nWhile the p-value (0.619) isn\'t definitive, the trend is negative (-$0.12). Even more interestingly, **visits increased, but conversion didn\'t**. This suggests the email drove traffic, but that traffic converted at a much lower rate than usual.\\n\\n<p>\\n  <img\\n    src={require(\'./imgs/women_visit_lift.png\').default}\\n    alt=\\"Women Visit Lift\\"\\n    style={{ maxHeight: \'80vh\', width: \'100%\', objectFit: \'contain\' }}\\n  />\\n</p>\\n\\nThis could be an important finding that prompts a fundamental review of the company\'s business strategy. Since sending emails actually led to a decrease in purchase amount for this segment, at the very least, emails should absolutely not be sent to them, and we must further investigate the reasons behind this negative impact. If you were the CEO of this company, you would never simply assume that removing this segment from the campaign solves every problem from the perspective of long-term business growth.\\n\\n\\nMany hypotheses come to mind. Since they are Phone-only customers, could this be an older demographic less comfortable with aggressive digital marketing?\\n\\nIf I were leading this analysis, my next step would be to investigate the **churn rate** for this specific segment.\\n*   **Is churn increasing?** This could signal problems with new product lines, competitors stealing market share, or issues with customer service in those regions.\\n*   **Is churn stable but high?** Perhaps the brand image has shifted too far towards men\'s merchandise, alienating this group.\\n\\n---\\n\\n## Conclusion\\n\\nSo, to answer the challenge:\\n\\n1.  **Which campaign is better?** The **Mens Campaign** is the overall winner.\\n2.  **Who should we target?** We can use a Causal Forest model to predict CATE for both campaigns and target users with the highest expected lift.\\n3.  **Who should we avoid?** We must strictly avoid the \\"Suburban / Phone-only / Low Spend\\" segment for the Womens campaign, as the email appears to destroy value there.\\n4.  **How to Improve?**\\n\\n    Based on our findings, here are three concrete ideas for the next iteration:\\n\\n    *   **Personalize Timing:** Since customers who purchased recently showed the higher CATE, we can personalize the *timing* of the campaign. Triggering emails shortly after a purchase\u2014rather than waiting for a batch campaign\u2014could capture this high engagement window.\\n    *   **Rethink the Channel:** For \\"Phone-only\\" users, digital channels like email clearly aren\'t landing. We should test analog approaches that align with their behavior, such as **direct mail (flyers)** or **phone calls**.\\n    *   **Dig Deeper with Surveys:** For the \\"Suburban / Phone-only / Low Spend\\" segment where we saw a negative impact, we need to go beyond behavioral data. Conducting **surveys** to understand their specific dissatisfactions will reveal why the campaign backfired and how to fix the relationship.\\n\\nBy moving beyond simple A/B testing to **Causal Inference**, we transformed a simple \\"Winner vs Loser\\" report into a nuanced strategy that optimizes effect while uncovering deep customer insights and \\n\\n\\n### Data Science Is Fun! Getting It Right Is What Makes It Valuable.\\nAchieve deeper understanding and higher-quality outputs in data science\u2014beyond your peers.\\n*If you want to explore the data yourself, grab the dataset and try reproducing these results in Allye!*\\n\\nYou can try [Allye Base](https://www.ai-allye.com/) for free."},{"id":"first-post","metadata":{"permalink":"/allye-doc/blog/first-post","editUrl":"https://github.com/Llenar70/allye-doc/tree/main/web-docs/blog/2026-01-10-welcome.md","source":"@site/blog/2026-01-10-welcome.md","title":"Hello! and Welcome","description":"Hello! We are Sho and Nao, the founders of Allye. We are incredibly excited to announce the release of Allye, the ideal product we\'ve always envisioned, built with the power of Generative AI.","date":"2026-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"Welcome","permalink":"/allye-doc/blog/tags/welcome","description":"Welcome posts"}],"readingTime":1.6,"hasTruncateMarker":false,"authors":[{"name":"Sho SEKINE","title":"Head of Applied Science at mercari, Principal Data Scientist at Fast Retailing, Co-founder AI Allye","url":"https://www.linkedin.com/in/sho-sekine-831339a3/","page":{"permalink":"/allye-doc/blog/authors/sho"},"socials":{"linkedin":"https://www.linkedin.com/in/sho-sekine-831339a3/","github":"https://github.com/LichtLab"},"imageURL":"https://media.licdn.com/dms/image/v2/C5603AQFBwY_R-HmADQ/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1643884685389?e=1769644800&v=beta&t=UwqhQSINs8z-g6Jplk9blG_E-waB_kjPBqcicWU8L-I","key":"sho"},{"name":"Nao SEKINE","title":"Ex-Recruit Holdings Senior AI Engineer, Co-founder AI Allye","url":"https://www.linkedin.com/in/nao-sekine-864663191/","page":{"permalink":"/allye-doc/blog/authors/nao"},"socials":{"linkedin":"https://www.linkedin.com/in/nao-sekine-864663191/","github":"https://github.com/Llenar70"},"imageURL":"https://media.licdn.com/dms/image/v2/C5103AQHsMOB9n5QsAA/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1566483021367?e=1769644800&v=beta&t=31zH1d2SG1aWZYiiUampTuTm-s2Y_F7A09pkRFjT1v4","key":"nao"}],"frontMatter":{"slug":"first-post","title":"Hello! and Welcome","authors":["sho","nao"],"tags":["welcome"]},"unlisted":false,"prevItem":{"title":"Personalize Email Campaign-MineThatData Challenge","permalink":"/allye-doc/blog/causal-post"}},"content":"Hello! We are Sho and Nao, the founders of Allye. We are incredibly excited to announce the release of Allye, the ideal product we\'ve always envisioned, built with the power of Generative AI.\\n\\n### Why we built Allye\\n\\nWith over 15 years of experience in Data Science, we\u2019ve seen the landscape evolve. Data Science knowledge is now essential not just for specialists, but also for Product Managers, Marketers, and Engineers.\\n\\nMeanwhile, business and research landscapes are becoming increasingly complex and personalized. As tasks multiply and the demand for efficiency grows, the time available for deep analysis shrinks. Additionally, with the widespread use of AI, the risk of data leakage is rising, making secure data handling more critical than ever.\\n\\nYet, the pressure to make correct, data-driven decisions remains high. We constantly need to \\"understand users,\\" \\"measure effects,\\" and \\"find strategies,\\" but are often buried in operational tasks.\\n\\n### What makes Allye unique\\n\\nWe built Allye to solve this dilemma. It is distinct from other no-code tools:\\n\\n1.  **Obsession with Speed:** Allye is optimized to process practical data sizes instantly. Speed is our UX promise.\\n2.  **Deep Integration with Python:** No-code is fast, but code is flexible. Allye bridges the gap\u2014the AI writes the Python code for you.\\n3.  **Comprehensive Causal Inference:** Understanding \\"why\\" is crucial for business decisions. That is why causal inference is our core analytics engine.\\n4.  **Local & Secure:** Allye runs locally and offline. Your data stays on your device, so you never have to worry about data leakage to AI.\\n\\nAllye Base is free. Check the [Quick Start](https://llenar70.github.io/allye-doc/docs/get-started/allye-quickstart-for-engineers) and start your journey now.\\n\\n---\\nLearn More: [Hands-on Tutorial](https://llenar70.github.io/allye-doc/docs/allye/adv-tutorial-overview)\\n\\n\\n### Data Science Is Fun! Getting It Right Is What Makes It Valuable.\\nAchieve deeper understanding and higher-quality outputs in data science\u2014beyond your peers."}]}}')}}]);